{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000, 784)\n",
      "(784,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "np.set_printoptions(linewidth=1000)\n",
    "import struct\n",
    "from array import array\n",
    "import os\n",
    "\n",
    "class MnistDataloader(object):\n",
    "    def __init__(self, training_images_filepath,training_labels_filepath,\n",
    "                 test_images_filepath, test_labels_filepath):\n",
    "        self.training_images_filepath = training_images_filepath\n",
    "        self.training_labels_filepath = training_labels_filepath\n",
    "        self.test_images_filepath = test_images_filepath\n",
    "        self.test_labels_filepath = test_labels_filepath\n",
    "    \n",
    "    def read_images_labels(self, images_filepath, labels_filepath):        \n",
    "        labels = []\n",
    "        with open(labels_filepath, 'rb') as file:\n",
    "            magic, size = struct.unpack(\">II\", file.read(8))\n",
    "            if magic != 2049:\n",
    "                raise ValueError('Magic number mismatch, expected 2049, got {}'.format(magic))\n",
    "            labels = array(\"B\", file.read())        \n",
    "        \n",
    "        with open(images_filepath, 'rb') as file:\n",
    "            magic, size, rows, cols = struct.unpack(\">IIII\", file.read(16))\n",
    "            if magic != 2051:\n",
    "                raise ValueError('Magic number mismatch, expected 2051, got {}'.format(magic))\n",
    "            image_data = array(\"B\", file.read())        \n",
    "        images = []\n",
    "        for i in range(size):\n",
    "            images.append([0] * rows * cols)\n",
    "        for i in range(size):\n",
    "            img = np.array(image_data[i * rows * cols:(i + 1) * rows * cols])\n",
    "            img = img.reshape(28, 28)\n",
    "            images[i][:] = img            \n",
    "        \n",
    "        return images, labels\n",
    "            \n",
    "    def load_data(self):\n",
    "        x_train, y_train = self.read_images_labels(self.training_images_filepath, self.training_labels_filepath)\n",
    "        x_test, y_test = self.read_images_labels(self.test_images_filepath, self.test_labels_filepath)\n",
    "        return (x_train, y_train),(x_test, y_test)\n",
    "\n",
    "input_path = './archive/'\n",
    "training_images_filepath = os.path.join(input_path, 'train-images-idx3-ubyte/train-images-idx3-ubyte')\n",
    "training_labels_filepath = os.path.join(input_path, 'train-labels-idx1-ubyte/train-labels-idx1-ubyte')\n",
    "test_images_filepath = os.path.join(input_path, 't10k-images-idx3-ubyte/t10k-images-idx3-ubyte')\n",
    "test_labels_filepath = os.path.join(input_path, 't10k-labels-idx1-ubyte/t10k-labels-idx1-ubyte')\n",
    "\n",
    "mnist_dataloader = MnistDataloader(training_images_filepath, training_labels_filepath, test_images_filepath, test_labels_filepath)\n",
    "(x_train, y_train), (x_test, y_test) = mnist_dataloader.load_data()\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "print(x_train.shape)\n",
    "x_train = x_train.reshape(x_train.shape[0], -1)\n",
    "print(x_train.shape)\n",
    "x_train = x_train.astype('float32')\n",
    "print(x_train[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   3.,  18.,  18.,  18., 126., 136., 175.,  26., 166., 255., 247., 127.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  30.,  36.,  94., 154., 170., 253., 253., 253., 253., 253., 225., 172., 253., 242., 195.,  64.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  49., 238., 253., 253., 253., 253., 253., 253., 253., 253., 251.,  93.,  82.,  82.,  56.,  39.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  18., 219., 253., 253., 253., 253., 253., 198., 182., 247., 241.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  80., 156., 107., 253., 253., 205.,  11.,   0.,  43., 154.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  14.,   1., 154., 253.,  90.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 139., 253., 190.,   2.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  11., 190., 253.,  70.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  35., 241., 225., 160., 108.,   1.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  81., 240., 253., 253., 119.,  25.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  45., 186., 253., 253., 150.,  27.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  16.,  93., 252., 253., 187.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 249., 253.,\n",
       "        249.,  64.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  46., 130., 183., 253., 253., 207.,   2.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  39., 148., 229., 253., 253., 253., 250., 182.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  24., 114., 221., 253., 253., 253., 253., 201.,  78.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  23.,  66., 213., 253., 253., 253., 253., 198.,  81.,   2.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  18., 171., 219., 253., 253., 253., 253., 195.,  80.,   9.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  55., 172., 226., 253., 253., 253., 253., 244., 133.,  11.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 136., 253., 253., 253., 212., 135., 132.,  16.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.], dtype=float32),\n",
       " 5)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0], y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784,)\n",
      "(784, 512)\n",
      "(512,)\n",
      "(512, 512)\n",
      "(512,)\n",
      "(512, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.09998494, 0.10002097, 0.10000858, 0.09999525, 0.10000187, 0.09999704, 0.10000026, 0.09998352, 0.0999949 , 0.10001267])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import time\n",
    "from typing import Any\n",
    "from scipy.special import softmax\n",
    "\n",
    "def ce_loss(y_pred, y_true):\n",
    "    loss = \n",
    "    return loss\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, embedding_dim, layer_dim, afunc):\n",
    "        self.W = np.random.rand(embedding_dim, layer_dim) * 0.002 - 0.001\n",
    "        self.B = np.zeros(layer_dim)\n",
    "        self.afunc = afunc\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'W: {self.W.shape}, B: {self.W.shape}, afunc: {self.afunc}'\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu(Z):\n",
    "        return np.array([max(0, Z[i]) for i in range(len(Z))])\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax(Z):\n",
    "        return np.array([math.exp(Z[i])/sum([math.exp(Z[j]) for j in range(len(Z))]) for i in range(len(Z))])\n",
    "    \n",
    "    def activation(self):\n",
    "        function_mapping = {\n",
    "            'relu': self.relu,\n",
    "            'softmax': softmax,\n",
    "        }\n",
    "        return function_mapping[self.afunc]\n",
    "\n",
    "    def forward(self, X):\n",
    "        print(X.shape)\n",
    "        print(self.W.shape)\n",
    "        self.Z = np.matmul(X, self.W) + self.B\n",
    "        return self.activation()(self.Z)\n",
    "\n",
    "class FNN:\n",
    "    def __init__(self, embedding_dim) -> None:\n",
    "        self.top_layer_dim = embedding_dim\n",
    "        self.layers = []\n",
    "    \n",
    "    def __str__(self):\n",
    "        string = '\\n'.join([layer.__str__() for layer in self.layers])\n",
    "        print(string)\n",
    "        if string != '':\n",
    "            return string\n",
    "        else:\n",
    "            return 'FNN object: no layers.'\n",
    "\n",
    "    def add(self, layer_dim, afunc):\n",
    "        layer = Layer(self.top_layer_dim, layer_dim, afunc)\n",
    "        self.top_layer_dim = layer_dim\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def print_struct(self):\n",
    "        print('number of layers:', len(self.layers))\n",
    "        for layer in self.layers:\n",
    "            print(layer.__str__())\n",
    "\n",
    "    def __call__(self, X) -> Any:\n",
    "        Y = X\n",
    "        for layer in self.layers:\n",
    "            Y = layer.forward(Y)\n",
    "        return Y\n",
    "\n",
    "\n",
    "fnn = FNN(x_train[0].shape[0])\n",
    "fnn.add(512, 'relu')\n",
    "fnn.add(512, 'relu')\n",
    "fnn.add(10, 'softmax')\n",
    "\n",
    "# fnn.print_struct()\n",
    "\n",
    "fnn(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
